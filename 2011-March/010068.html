<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Opensim-dev] networking issues
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/opensim-dev/2011-March/index.html" >
   <LINK REL="made" HREF="mailto:opensim-dev%40lists.berlios.de?Subject=Re%3A%20%5BOpensim-dev%5D%20networking%20issues&In-Reply-To=%3CBANLkTin7y_o%2BaADvBDzgt%3DW28_Bxn%2BkWFw%40mail.gmail.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="010078.html">
   <LINK REL="Next"  HREF="010069.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Opensim-dev] networking issues</H1>
    <B>Dahlia Trimble</B> 
    <A HREF="mailto:opensim-dev%40lists.berlios.de?Subject=Re%3A%20%5BOpensim-dev%5D%20networking%20issues&In-Reply-To=%3CBANLkTin7y_o%2BaADvBDzgt%3DW28_Bxn%2BkWFw%40mail.gmail.com%3E"
       TITLE="[Opensim-dev] networking issues">dahliatrimble at gmail.com
       </A><BR>
    <I>Mon Mar 28 20:49:55 CEST 2011</I>
    <P><UL>
        <LI>Previous message: <A HREF="010078.html">[Opensim-dev] networking issues
</A></li>
        <LI>Next message: <A HREF="010069.html">[Opensim-dev] networking issues
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#10068">[ date ]</a>
              <a href="thread.html#10068">[ thread ]</a>
              <a href="subject.html#10068">[ subject ]</a>
              <a href="author.html#10068">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>a couple thoughts..

Perhaps resend timeout period could be a function of throttle setting and/or
measured packet acknowledgement time per-client? (provided we measure it).
That may prevent excessive resend processing that may not be necessary.

On the distance prioritization, could small changed in object translations
be discarded from the prioritization queues/resend buffers for distant
objects when new updates occur for those objects? Small changes may not be
noticeable from the viewer perspective anyway.


On Mon, Mar 28, 2011 at 10:48 AM, Teravus Ovares &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/opensim-dev">teravus at gmail.com</A>&gt; wrote:

&gt;<i> Here are a few facts that I've personally discovered while working
</I>&gt;<i> with LLClientView.
</I>&gt;<i>
</I>&gt;<i> 1. It has been noted that people with poor connections to the
</I>&gt;<i> simulator do consume more bandwidth, cpu, and have a generally worse
</I>&gt;<i> experience.   This has been tested and profiled extensively.    This
</I>&gt;<i> may seem like a small issue because what it's doing is so basic...
</I>&gt;<i> however the frequency in which this occurs is a real cause of
</I>&gt;<i> performance issues.
</I>&gt;<i>
</I>&gt;<i> 2. It's also noted that the CPU used in these cases reduces the CPU
</I>&gt;<i> available to the rest of the simulator resulting in a lower quality of
</I>&gt;<i> service for the rest of the people on the simulator.
</I>&gt;<i> This has been seen in the profiling and has been qualitatively
</I>&gt;<i> observed by a large number of users connected and everything is OK and
</I>&gt;<i> then a 'problem connection' user connecting causing a wide range of
</I>&gt;<i> issues.
</I>&gt;<i>
</I>&gt;<i> 3. It's also noted that lowering the outgoing UDP packet throttles
</I>&gt;<i> beyond a certain point results in perpetual queuing and resends.
</I>&gt;<i> This was tested by using a throttle multiplier last year that was
</I>&gt;<i> implemented by justincc.  I'm not sure if the multiplier is still
</I>&gt;<i> there.   It's most easily seen with image packets.   Again, I note
</I>&gt;<i> that the packets are not rebuilt going from the regular outbound queue
</I>&gt;<i> to the resend queue.    The resend queue is /supposed/ to be used to
</I>&gt;<i> quickly get data that is essential to the client after attempting to
</I>&gt;<i> send once already.   The UDP spec declares the maximum resend to be 2
</I>&gt;<i> times, however there has been some considerable debate on whether or
</I>&gt;<i> not OpenSimulator should follow that specific specification item
</I>&gt;<i> leading to a configuration option to enable perpetual resends
</I>&gt;<i> (Implemented by Melanie).  The configuration item was named similar
</I>&gt;<i> to, 'reliable is important' or something like that.   I'm not sure if
</I>&gt;<i> the configuration item survived the many revisions however I suspect
</I>&gt;<i> that it did.
</I>&gt;<i>
</I>&gt;<i> 4. It's also noted that raising the packet throttles beyond what the
</I>&gt;<i> connection can support results in resending almost every packet the
</I>&gt;<i> maximum amount of times before the limit is reached.
</I>&gt;<i> This is easily reproducible by setting the connection (in the client)
</I>&gt;<i> to the maximum and connecting to a region that you've never been to
</I>&gt;<i> before on a sub par connection.   Before the client adjusts and
</I>&gt;<i> requests a lower throttle setting there's massive data loss and
</I>&gt;<i> massive re-queuing.
</I>&gt;<i>
</I>&gt;<i> 5. The client tries to adjust the throttle settings based on network
</I>&gt;<i> conditions.   This can be observed by monitoring the packet that sets
</I>&gt;<i> the throttles and dragging the bar to maximum.   After a certain
</I>&gt;<i> amount of resends, the client will call the set throttle packet with
</I>&gt;<i> reduced settings (some argue that it doesn't do that fast enough).
</I>&gt;<i>
</I>&gt;<i> 6. A user who has connected previously to the simulator will use less
</I>&gt;<i> resources then a user who has never connected to the simulator.  (this
</I>&gt;<i> is mostly because of the image cache on the client).    Any client
</I>&gt;<i> that uses CAPS images will use less resources then one that uses
</I>&gt;<i> LLUDP.
</I>&gt;<i>
</I>&gt;<i> When working with the packet queues, it's essential to understand
</I>&gt;<i> those 6 observations.   Even though, the place where you tend to see
</I>&gt;<i> the issues with queuing is the image queue over LLUDP, the principles
</I>&gt;<i> apply to all of the udp queues.
</I>&gt;<i>
</I>&gt;<i> Regards
</I>&gt;<i>
</I>&gt;<i> Teravus
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> On Mon, Mar 28, 2011 at 1:00 PM, Mic Bowman &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/opensim-dev">cmickeyb at gmail.com</A>&gt; wrote:
</I>&gt;<i> &gt; Over the last several weeks, Dan Lake &amp; I have been looking some of the
</I>&gt;<i> &gt; networking performance issues in opensim. As always, our concerns are
</I>&gt;<i> with
</I>&gt;<i> &gt; the problems caused by very complex scenes with very large numbers of
</I>&gt;<i> &gt; avatars. However, I think some of the issues we have found will generally
</I>&gt;<i> &gt; improve networking with OpenSim. Since the behavior represents a fairly
</I>&gt;<i> &gt; significant change in behavior (though the number of lines of code is not
</I>&gt;<i> &gt; great), I'm going to put this into a separate branch for testing (called
</I>&gt;<i> &gt; queuetest) in the opensim git repository.
</I>&gt;<i> &gt; We've found several problems with the current
</I>&gt;<i> &gt; networking/prioritization code.
</I>&gt;<i> &gt; * Reprioritization is completely broken for SceneObjectParts. On
</I>&gt;<i> &gt; reprioritization, the current code uses the localid stored in the scene
</I>&gt;<i> &gt; Entities list but since the scene does not store the localid for SOPs,
</I>&gt;<i> that
</I>&gt;<i> &gt; attempt always fails. So the original priority of the SOP continues to be
</I>&gt;<i> &gt; used. This could be the cause of some problems since the initial
</I>&gt;<i> &gt; prioritization assumes position 128,128. I don't understand all the
</I>&gt;<i> possible
</I>&gt;<i> &gt; ramifications, but suffice it to say, using the localid is causing
</I>&gt;<i> &gt; problems.
</I>&gt;<i> &gt; Fix: The sceneentity is already stored in the update, just use that
</I>&gt;<i> instead
</I>&gt;<i> &gt; of the localid.
</I>&gt;<i> &gt; * We currently pull (by default) 100 entity updates from the entityupdate
</I>&gt;<i> &gt; queue and convert them into packets. Once converted into packets, they
</I>&gt;<i> are
</I>&gt;<i> &gt; then queued again for transmissions. This is a bad thing. Under any kind
</I>&gt;<i> of
</I>&gt;<i> &gt; load, we've measured the time in the packet queue to be up to many
</I>&gt;<i> &gt; hundreds/thousands of milliseconds (and to be highly variable). When an
</I>&gt;<i> &gt; object changes one property and then doesn't change it again, the time in
</I>&gt;<i> &gt; the packet queue is largely irrelevant. However, if the object is
</I>&gt;<i> &gt; continuously changing (an avatar changing position, a physical object
</I>&gt;<i> &gt; moving, etc) then the conversion from a entity update to a packet
</I>&gt;<i> &quot;freezes&quot;
</I>&gt;<i> &gt; the properties to be sent. If the object is continuously changing, then
</I>&gt;<i> with
</I>&gt;<i> &gt; fairly high probability, the packet contains old data (the properties of
</I>&gt;<i> the
</I>&gt;<i> &gt; entity from the point at which it was converted into a packet).
</I>&gt;<i> &gt; The real problem is that, in theory, to improve the efficiency of the
</I>&gt;<i> &gt; packets (fill up each message) we are grabbing big chunks of updates.
</I>&gt;<i> Under
</I>&gt;<i> &gt; load, that causes queuing at the packet layer which makes updates stale.
</I>&gt;<i> &gt; That is... queuing at the packet layer is BAD.
</I>&gt;<i> &gt; Fix: We implemented an adaptive algorithm for the number of updates to
</I>&gt;<i> grab
</I>&gt;<i> &gt; with each pass. We set a target time of 200ms for each iteration. That
</I>&gt;<i> &gt; means, we are trying to bound the maximum age of any update in the packet
</I>&gt;<i> &gt; queue to 200ms. The adaptive algorithm looks a lot like a TCP slow start:
</I>&gt;<i> &gt; every time we complete an iteration (flush the packet queue) in less than
</I>&gt;<i> &gt; 200ms we increase linearly the number of updates we take in the next
</I>&gt;<i> &gt; iteration (add 5 to the count) and when we don't make it back in 200ms,
</I>&gt;<i> we
</I>&gt;<i> &gt; drop the number we take quadratically (cut the number in half). In our
</I>&gt;<i> &gt; experiments with large numbers of moving avatars, this algorithm works
</I>&gt;<i> &gt; *very* well. The number of updates taken per iteration stabilizes very
</I>&gt;<i> &gt; quickly and the response time is dramatically improved (no &quot;snap back&quot; on
</I>&gt;<i> &gt; avatars, for example). One difference from the traditional slow start...
</I>&gt;<i> &gt; since the number of &quot;static&quot; items in the queue is very high when a
</I>&gt;<i> client
</I>&gt;<i> &gt; first enters a region, we start with the number of updates taken at 500.
</I>&gt;<i> &gt; that gets the static items out of the queue quickly (and delay doesn't
</I>&gt;<i> &gt; matter as much) and the number taken is generally stable before the
</I>&gt;<i> &gt; login/teleport screen even goes away.
</I>&gt;<i> &gt; * The current prioritization queue can lead to update starvation. The
</I>&gt;<i> &gt; prioritization algorithm dumps all entity updates into a single ordered
</I>&gt;<i> &gt; queue. Lets say you have several hundred avatars moving around in a
</I>&gt;<i> scene.
</I>&gt;<i> &gt; Since we take a limited number of updates from the queue in each
</I>&gt;<i> iteration,
</I>&gt;<i> &gt; we will take only the updates for the &quot;closest&quot; (highest priority)
</I>&gt;<i> avatars.
</I>&gt;<i> &gt; However, since those avatars continue to move, they are re-inserted into
</I>&gt;<i> the
</I>&gt;<i> &gt; priority queue *ahead* of the updates that were already there. So...
</I>&gt;<i> unless
</I>&gt;<i> &gt; the queue can be completely emptied each iteration or the priority of the
</I>&gt;<i> &gt; &quot;distant&quot; (low priority) avatars changes, those avatars will never be
</I>&gt;<i> &gt; updated.
</I>&gt;<i> &gt; Fix: We converted the single priority queue into multiple priority queues
</I>&gt;<i> &gt; and use fair queuing to retrieve updates from each. Here's how it works
</I>&gt;<i> &gt; (more or less)... the current metrics (all of the current prioritization
</I>&gt;<i> &gt; algorithms use distance at some point for prioritization) compute a
</I>&gt;<i> distance
</I>&gt;<i> &gt; from the avatar/camera to an object. We take the log of that distance and
</I>&gt;<i> &gt; use that as the index for the queue where we place the update. So close
</I>&gt;<i> &gt; things go into the highest priority queue and distant things go into the
</I>&gt;<i> &gt; lowest priority queue. Since the area covered by a priority queue grows
</I>&gt;<i> as
</I>&gt;<i> &gt; the square of the radius, the distant (lowest priority queues) will have
</I>&gt;<i> the
</I>&gt;<i> &gt; most objects while the highest priority queues will have a small number
</I>&gt;<i> of
</I>&gt;<i> &gt; objects. Inside each priority queue, we order the updates by the time in
</I>&gt;<i> &gt; which they entered the queue. Then we pull a fixed number of updates from
</I>&gt;<i> &gt; each priority queue each iteration. The result is that local updates get
</I>&gt;<i> a
</I>&gt;<i> &gt; high fraction of the outgoing bandwidth but distant updates are
</I>&gt;<i> guaranteed
</I>&gt;<i> &gt; to get at least &quot;some&quot; of the bandwidth. No starvation. The current
</I>&gt;<i> &gt; prioritization algorithm we implemented is a modification of the &quot;best
</I>&gt;<i> &gt; avatar responsiveness&quot; and &quot;front back&quot; in that we use root prim location
</I>&gt;<i> &gt; for child prims and the priority of updates &quot;in back&quot; of the avatar is
</I>&gt;<i> lower
</I>&gt;<i> &gt; than updates &quot;in front&quot;. Our experiments show that the fair queuing does
</I>&gt;<i> &gt; drain the update queue AND continues to provide a disproportionately high
</I>&gt;<i> &gt; percentage of the bw to &quot;close&quot; updates.
</I>&gt;<i> &gt; One other note on this... we should be able to improve the performance of
</I>&gt;<i> &gt; reprioritization with this approach. If we know the distance an avatar
</I>&gt;<i> has
</I>&gt;<i> &gt; moved, we only have to reprioritize objects that might have changed
</I>&gt;<i> priority
</I>&gt;<i> &gt; queues. Haven't implemented this yet but have some ideas for how to do
</I>&gt;<i> it.
</I>&gt;<i> &gt; * The resend queue is evil. When an update packet is sent (they are
</I>&gt;<i> marked
</I>&gt;<i> &gt; reliable) it is moved to a queue to await acknowledgement. If no
</I>&gt;<i> &gt; acknowledgement is received (in time), the packet is retransmitted and
</I>&gt;<i> the
</I>&gt;<i> &gt; wait time is doubled and so on... What that means is that a resend
</I>&gt;<i> packets
</I>&gt;<i> &gt; in a scene that is rapidly changing will often contain updates that are
</I>&gt;<i> &gt; outdated. That is, when we resend the packet, we are just resending old
</I>&gt;<i> data
</I>&gt;<i> &gt; (and if you're having a lot of resends that means you already have a bad
</I>&gt;<i> &gt; connection &amp; now you're filling it up with useless data).
</I>&gt;<i> &gt; Fix: this isn't implemented yet (help would be appreciated)... we think
</I>&gt;<i> that
</I>&gt;<i> &gt; instead of saving packets for resend... a better solution would be to
</I>&gt;<i> keep
</I>&gt;<i> &gt; the entity updates that went into the packet. if we don't receive an ack
</I>&gt;<i> in
</I>&gt;<i> &gt; time, then put the entity updates back into the entity update queue (with
</I>&gt;<i> &gt; entry time from their original enqueuing). That would ensure that we send
</I>&gt;<i> an
</I>&gt;<i> &gt; update for the object &amp; that the data sent is the most recent.
</I>&gt;<i> &gt; * One final note... per client bandwidth throttles seem to work very
</I>&gt;<i> well.
</I>&gt;<i> &gt; however, our experiments with per-simulator throttles was not positive.
</I>&gt;<i> it
</I>&gt;<i> &gt; appeared that a small number of clients was consuming all of the bw
</I>&gt;<i> &gt; available to the simulator and the rest were starved. Haven't looked into
</I>&gt;<i> &gt; this any more.
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; So...
</I>&gt;<i> &gt; Feedback appreciated... there is some logging code (disabled) in the
</I>&gt;<i> branch;
</I>&gt;<i> &gt; real data would be great. And help testing. there are a number of
</I>&gt;<i> &gt; attachment, deletes and so on that i'm not sure work correctly.
</I>&gt;<i> &gt; --mic
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; _______________________________________________
</I>&gt;<i> &gt; Opensim-dev mailing list
</I>&gt;<i> &gt; <A HREF="https://lists.berlios.de/mailman/listinfo/opensim-dev">Opensim-dev at lists.berlios.de</A>
</I>&gt;<i> &gt; <A HREF="https://lists.berlios.de/mailman/listinfo/opensim-dev">https://lists.berlios.de/mailman/listinfo/opensim-dev</A>
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> _______________________________________________
</I>&gt;<i> Opensim-dev mailing list
</I>&gt;<i> <A HREF="https://lists.berlios.de/mailman/listinfo/opensim-dev">Opensim-dev at lists.berlios.de</A>
</I>&gt;<i> <A HREF="https://lists.berlios.de/mailman/listinfo/opensim-dev">https://lists.berlios.de/mailman/listinfo/opensim-dev</A>
</I>&gt;<i>
</I>-------------- next part --------------
An HTML attachment was scrubbed...
URL: &lt;<A HREF="https://lists.berlios.de/pipermail/opensim-dev/attachments/20110328/cd835043/attachment.html">https://lists.berlios.de/pipermail/opensim-dev/attachments/20110328/cd835043/attachment.html</A>&gt;
</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="010078.html">[Opensim-dev] networking issues
</A></li>
	<LI>Next message: <A HREF="010069.html">[Opensim-dev] networking issues
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#10068">[ date ]</a>
              <a href="thread.html#10068">[ thread ]</a>
              <a href="subject.html#10068">[ subject ]</a>
              <a href="author.html#10068">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/opensim-dev">More information about the Opensim-dev
mailing list</a><br>
</body></html>
