<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Opensim-dev] networking issues
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/opensim-dev/2011-March/index.html" >
   <LINK REL="made" HREF="mailto:opensim-dev%40lists.berlios.de?Subject=Re%3A%20%5BOpensim-dev%5D%20networking%20issues&In-Reply-To=%3CBANLkTinWNpyZwU6-wnp6HqQ17GEhjbQ5jA%40mail.gmail.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="010077.html">
   <LINK REL="Next"  HREF="010066.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Opensim-dev] networking issues</H1>
    <B>Dahlia Trimble</B> 
    <A HREF="mailto:opensim-dev%40lists.berlios.de?Subject=Re%3A%20%5BOpensim-dev%5D%20networking%20issues&In-Reply-To=%3CBANLkTinWNpyZwU6-wnp6HqQ17GEhjbQ5jA%40mail.gmail.com%3E"
       TITLE="[Opensim-dev] networking issues">dahliatrimble at gmail.com
       </A><BR>
    <I>Mon Mar 28 21:48:43 CEST 2011</I>
    <P><UL>
        <LI>Previous message: <A HREF="010077.html">[Opensim-dev] networking issues
</A></li>
        <LI>Next message: <A HREF="010066.html">[Opensim-dev] networking issues
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#10079">[ date ]</a>
              <a href="thread.html#10079">[ thread ]</a>
              <a href="subject.html#10079">[ subject ]</a>
              <a href="author.html#10079">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>I'm not sure how often the viewer adjusts but I have seen message on the
linux viewer console which mentioned throttle adjustments. At the time they
seemed quite frequent, perhaps in tens of seconds between messages.

On Mon, Mar 28, 2011 at 12:34 PM, Mic Bowman &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/opensim-dev">cmickeyb at gmail.com</A>&gt; wrote:

&gt;<i> comments below...
</I>&gt;<i>
</I>&gt;<i> On Mon, Mar 28, 2011 at 11:49 AM, Dahlia Trimble &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/opensim-dev">dahliatrimble at gmail.com</A>&gt;wrote:
</I>&gt;<i>
</I>&gt;&gt;<i> a couple thoughts..
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Perhaps resend timeout period could be a function of throttle setting
</I>&gt;&gt;<i> and/or measured packet acknowledgement time per-client? (provided we measure
</I>&gt;&gt;<i> it). That may prevent excessive resend processing that may not be necessary.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;<i> how/when does the viewer change its throttles? i have an &quot;interesting&quot;
</I>&gt;<i> network connection to the public internet from intel (very long cable --&gt;
</I>&gt;<i> periodic high error rates)... i've seen 300% packet loss rates (explain that
</I>&gt;<i> one!)... but never see a &quot;throttle down&quot; packet that would drop the update
</I>&gt;<i> rates back to a range where the network can reasonably handle it.
</I>&gt;<i>
</I>&gt;<i> dan &amp; i were talking today about how to adjust the throttles simulator-side
</I>&gt;<i> when we start to see large number of packet retransmissions.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;&gt;<i> On the distance prioritization, could small changed in object translations
</I>&gt;&gt;<i> be discarded from the prioritization queues/resend buffers for distant
</I>&gt;&gt;<i> objects when new updates occur for those objects? Small changes may not be
</I>&gt;&gt;<i> noticeable from the viewer perspective anyway.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;<i> dropping the update altogether is one approach but seems like it could mess
</I>&gt;<i> up state. deprioritizing based on the magnitude of change seems like a
</I>&gt;<i> better approach?? i think meru uses a version of angular distance. however,
</I>&gt;<i> the computation of that is pretty expensive.
</I>&gt;<i>
</I>&gt;<i> with deprioritizing... we see updates accumulating. since the entity stays
</I>&gt;<i> in the queue longer, you tend to accumulate greater and greater changes.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> On Mon, Mar 28, 2011 at 10:48 AM, Teravus Ovares &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/opensim-dev">teravus at gmail.com</A>&gt;wrote:
</I>&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Here are a few facts that I've personally discovered while working
</I>&gt;&gt;&gt;<i> with LLClientView.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> 1. It has been noted that people with poor connections to the
</I>&gt;&gt;&gt;<i> simulator do consume more bandwidth, cpu, and have a generally worse
</I>&gt;&gt;&gt;<i> experience.   This has been tested and profiled extensively.    This
</I>&gt;&gt;&gt;<i> may seem like a small issue because what it's doing is so basic...
</I>&gt;&gt;&gt;<i> however the frequency in which this occurs is a real cause of
</I>&gt;&gt;&gt;<i> performance issues.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> 2. It's also noted that the CPU used in these cases reduces the CPU
</I>&gt;&gt;&gt;<i> available to the rest of the simulator resulting in a lower quality of
</I>&gt;&gt;&gt;<i> service for the rest of the people on the simulator.
</I>&gt;&gt;&gt;<i> This has been seen in the profiling and has been qualitatively
</I>&gt;&gt;&gt;<i> observed by a large number of users connected and everything is OK and
</I>&gt;&gt;&gt;<i> then a 'problem connection' user connecting causing a wide range of
</I>&gt;&gt;&gt;<i> issues.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> 3. It's also noted that lowering the outgoing UDP packet throttles
</I>&gt;&gt;&gt;<i> beyond a certain point results in perpetual queuing and resends.
</I>&gt;&gt;&gt;<i> This was tested by using a throttle multiplier last year that was
</I>&gt;&gt;&gt;<i> implemented by justincc.  I'm not sure if the multiplier is still
</I>&gt;&gt;&gt;<i> there.   It's most easily seen with image packets.   Again, I note
</I>&gt;&gt;&gt;<i> that the packets are not rebuilt going from the regular outbound queue
</I>&gt;&gt;&gt;<i> to the resend queue.    The resend queue is /supposed/ to be used to
</I>&gt;&gt;&gt;<i> quickly get data that is essential to the client after attempting to
</I>&gt;&gt;&gt;<i> send once already.   The UDP spec declares the maximum resend to be 2
</I>&gt;&gt;&gt;<i> times, however there has been some considerable debate on whether or
</I>&gt;&gt;&gt;<i> not OpenSimulator should follow that specific specification item
</I>&gt;&gt;&gt;<i> leading to a configuration option to enable perpetual resends
</I>&gt;&gt;&gt;<i> (Implemented by Melanie).  The configuration item was named similar
</I>&gt;&gt;&gt;<i> to, 'reliable is important' or something like that.   I'm not sure if
</I>&gt;&gt;&gt;<i> the configuration item survived the many revisions however I suspect
</I>&gt;&gt;&gt;<i> that it did.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> 4. It's also noted that raising the packet throttles beyond what the
</I>&gt;&gt;&gt;<i> connection can support results in resending almost every packet the
</I>&gt;&gt;&gt;<i> maximum amount of times before the limit is reached.
</I>&gt;&gt;&gt;<i> This is easily reproducible by setting the connection (in the client)
</I>&gt;&gt;&gt;<i> to the maximum and connecting to a region that you've never been to
</I>&gt;&gt;&gt;<i> before on a sub par connection.   Before the client adjusts and
</I>&gt;&gt;&gt;<i> requests a lower throttle setting there's massive data loss and
</I>&gt;&gt;&gt;<i> massive re-queuing.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> 5. The client tries to adjust the throttle settings based on network
</I>&gt;&gt;&gt;<i> conditions.   This can be observed by monitoring the packet that sets
</I>&gt;&gt;&gt;<i> the throttles and dragging the bar to maximum.   After a certain
</I>&gt;&gt;&gt;<i> amount of resends, the client will call the set throttle packet with
</I>&gt;&gt;&gt;<i> reduced settings (some argue that it doesn't do that fast enough).
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> 6. A user who has connected previously to the simulator will use less
</I>&gt;&gt;&gt;<i> resources then a user who has never connected to the simulator.  (this
</I>&gt;&gt;&gt;<i> is mostly because of the image cache on the client).    Any client
</I>&gt;&gt;&gt;<i> that uses CAPS images will use less resources then one that uses
</I>&gt;&gt;&gt;<i> LLUDP.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> When working with the packet queues, it's essential to understand
</I>&gt;&gt;&gt;<i> those 6 observations.   Even though, the place where you tend to see
</I>&gt;&gt;&gt;<i> the issues with queuing is the image queue over LLUDP, the principles
</I>&gt;&gt;&gt;<i> apply to all of the udp queues.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Regards
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Teravus
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> On Mon, Mar 28, 2011 at 1:00 PM, Mic Bowman &lt;<A HREF="https://lists.berlios.de/mailman/listinfo/opensim-dev">cmickeyb at gmail.com</A>&gt; wrote:
</I>&gt;&gt;&gt;<i> &gt; Over the last several weeks, Dan Lake &amp; I have been looking some of the
</I>&gt;&gt;&gt;<i> &gt; networking performance issues in opensim. As always, our concerns are
</I>&gt;&gt;&gt;<i> with
</I>&gt;&gt;&gt;<i> &gt; the problems caused by very complex scenes with very large numbers of
</I>&gt;&gt;&gt;<i> &gt; avatars. However, I think some of the issues we have found will
</I>&gt;&gt;&gt;<i> generally
</I>&gt;&gt;&gt;<i> &gt; improve networking with OpenSim. Since the behavior represents a fairly
</I>&gt;&gt;&gt;<i> &gt; significant change in behavior (though the number of lines of code is
</I>&gt;&gt;&gt;<i> not
</I>&gt;&gt;&gt;<i> &gt; great), I'm going to put this into a separate branch for testing
</I>&gt;&gt;&gt;<i> (called
</I>&gt;&gt;&gt;<i> &gt; queuetest) in the opensim git repository.
</I>&gt;&gt;&gt;<i> &gt; We've found several problems with the current
</I>&gt;&gt;&gt;<i> &gt; networking/prioritization code.
</I>&gt;&gt;&gt;<i> &gt; * Reprioritization is completely broken for SceneObjectParts. On
</I>&gt;&gt;&gt;<i> &gt; reprioritization, the current code uses the localid stored in the scene
</I>&gt;&gt;&gt;<i> &gt; Entities list but since the scene does not store the localid for SOPs,
</I>&gt;&gt;&gt;<i> that
</I>&gt;&gt;&gt;<i> &gt; attempt always fails. So the original priority of the SOP continues to
</I>&gt;&gt;&gt;<i> be
</I>&gt;&gt;&gt;<i> &gt; used. This could be the cause of some problems since the initial
</I>&gt;&gt;&gt;<i> &gt; prioritization assumes position 128,128. I don't understand all the
</I>&gt;&gt;&gt;<i> possible
</I>&gt;&gt;&gt;<i> &gt; ramifications, but suffice it to say, using the localid is causing
</I>&gt;&gt;&gt;<i> &gt; problems.
</I>&gt;&gt;&gt;<i> &gt; Fix: The sceneentity is already stored in the update, just use that
</I>&gt;&gt;&gt;<i> instead
</I>&gt;&gt;&gt;<i> &gt; of the localid.
</I>&gt;&gt;&gt;<i> &gt; * We currently pull (by default) 100 entity updates from the
</I>&gt;&gt;&gt;<i> entityupdate
</I>&gt;&gt;&gt;<i> &gt; queue and convert them into packets. Once converted into packets, they
</I>&gt;&gt;&gt;<i> are
</I>&gt;&gt;&gt;<i> &gt; then queued again for transmissions. This is a bad thing. Under any
</I>&gt;&gt;&gt;<i> kind of
</I>&gt;&gt;&gt;<i> &gt; load, we've measured the time in the packet queue to be up to many
</I>&gt;&gt;&gt;<i> &gt; hundreds/thousands of milliseconds (and to be highly variable). When an
</I>&gt;&gt;&gt;<i> &gt; object changes one property and then doesn't change it again, the time
</I>&gt;&gt;&gt;<i> in
</I>&gt;&gt;&gt;<i> &gt; the packet queue is largely irrelevant. However, if the object is
</I>&gt;&gt;&gt;<i> &gt; continuously changing (an avatar changing position, a physical object
</I>&gt;&gt;&gt;<i> &gt; moving, etc) then the conversion from a entity update to a packet
</I>&gt;&gt;&gt;<i> &quot;freezes&quot;
</I>&gt;&gt;&gt;<i> &gt; the properties to be sent. If the object is continuously changing, then
</I>&gt;&gt;&gt;<i> with
</I>&gt;&gt;&gt;<i> &gt; fairly high probability, the packet contains old data (the properties
</I>&gt;&gt;&gt;<i> of the
</I>&gt;&gt;&gt;<i> &gt; entity from the point at which it was converted into a packet).
</I>&gt;&gt;&gt;<i> &gt; The real problem is that, in theory, to improve the efficiency of the
</I>&gt;&gt;&gt;<i> &gt; packets (fill up each message) we are grabbing big chunks of updates.
</I>&gt;&gt;&gt;<i> Under
</I>&gt;&gt;&gt;<i> &gt; load, that causes queuing at the packet layer which makes updates
</I>&gt;&gt;&gt;<i> stale.
</I>&gt;&gt;&gt;<i> &gt; That is... queuing at the packet layer is BAD.
</I>&gt;&gt;&gt;<i> &gt; Fix: We implemented an adaptive algorithm for the number of updates to
</I>&gt;&gt;&gt;<i> grab
</I>&gt;&gt;&gt;<i> &gt; with each pass. We set a target time of 200ms for each iteration. That
</I>&gt;&gt;&gt;<i> &gt; means, we are trying to bound the maximum age of any update in the
</I>&gt;&gt;&gt;<i> packet
</I>&gt;&gt;&gt;<i> &gt; queue to 200ms. The adaptive algorithm looks a lot like a TCP slow
</I>&gt;&gt;&gt;<i> start:
</I>&gt;&gt;&gt;<i> &gt; every time we complete an iteration (flush the packet queue) in less
</I>&gt;&gt;&gt;<i> than
</I>&gt;&gt;&gt;<i> &gt; 200ms we increase linearly the number of updates we take in the next
</I>&gt;&gt;&gt;<i> &gt; iteration (add 5 to the count) and when we don't make it back in 200ms,
</I>&gt;&gt;&gt;<i> we
</I>&gt;&gt;&gt;<i> &gt; drop the number we take quadratically (cut the number in half). In our
</I>&gt;&gt;&gt;<i> &gt; experiments with large numbers of moving avatars, this algorithm works
</I>&gt;&gt;&gt;<i> &gt; *very* well. The number of updates taken per iteration stabilizes very
</I>&gt;&gt;&gt;<i> &gt; quickly and the response time is dramatically improved (no &quot;snap back&quot;
</I>&gt;&gt;&gt;<i> on
</I>&gt;&gt;&gt;<i> &gt; avatars, for example). One difference from the traditional slow
</I>&gt;&gt;&gt;<i> start...
</I>&gt;&gt;&gt;<i> &gt; since the number of &quot;static&quot; items in the queue is very high when a
</I>&gt;&gt;&gt;<i> client
</I>&gt;&gt;&gt;<i> &gt; first enters a region, we start with the number of updates taken at
</I>&gt;&gt;&gt;<i> 500.
</I>&gt;&gt;&gt;<i> &gt; that gets the static items out of the queue quickly (and delay doesn't
</I>&gt;&gt;&gt;<i> &gt; matter as much) and the number taken is generally stable before the
</I>&gt;&gt;&gt;<i> &gt; login/teleport screen even goes away.
</I>&gt;&gt;&gt;<i> &gt; * The current prioritization queue can lead to update starvation. The
</I>&gt;&gt;&gt;<i> &gt; prioritization algorithm dumps all entity updates into a single ordered
</I>&gt;&gt;&gt;<i> &gt; queue. Lets say you have several hundred avatars moving around in a
</I>&gt;&gt;&gt;<i> scene.
</I>&gt;&gt;&gt;<i> &gt; Since we take a limited number of updates from the queue in each
</I>&gt;&gt;&gt;<i> iteration,
</I>&gt;&gt;&gt;<i> &gt; we will take only the updates for the &quot;closest&quot; (highest priority)
</I>&gt;&gt;&gt;<i> avatars.
</I>&gt;&gt;&gt;<i> &gt; However, since those avatars continue to move, they are re-inserted
</I>&gt;&gt;&gt;<i> into the
</I>&gt;&gt;&gt;<i> &gt; priority queue *ahead* of the updates that were already there. So...
</I>&gt;&gt;&gt;<i> unless
</I>&gt;&gt;&gt;<i> &gt; the queue can be completely emptied each iteration or the priority of
</I>&gt;&gt;&gt;<i> the
</I>&gt;&gt;&gt;<i> &gt; &quot;distant&quot; (low priority) avatars changes, those avatars will never be
</I>&gt;&gt;&gt;<i> &gt; updated.
</I>&gt;&gt;&gt;<i> &gt; Fix: We converted the single priority queue into multiple priority
</I>&gt;&gt;&gt;<i> queues
</I>&gt;&gt;&gt;<i> &gt; and use fair queuing to retrieve updates from each. Here's how it works
</I>&gt;&gt;&gt;<i> &gt; (more or less)... the current metrics (all of the current
</I>&gt;&gt;&gt;<i> prioritization
</I>&gt;&gt;&gt;<i> &gt; algorithms use distance at some point for prioritization) compute a
</I>&gt;&gt;&gt;<i> distance
</I>&gt;&gt;&gt;<i> &gt; from the avatar/camera to an object. We take the log of that distance
</I>&gt;&gt;&gt;<i> and
</I>&gt;&gt;&gt;<i> &gt; use that as the index for the queue where we place the update. So close
</I>&gt;&gt;&gt;<i> &gt; things go into the highest priority queue and distant things go into
</I>&gt;&gt;&gt;<i> the
</I>&gt;&gt;&gt;<i> &gt; lowest priority queue. Since the area covered by a priority queue grows
</I>&gt;&gt;&gt;<i> as
</I>&gt;&gt;&gt;<i> &gt; the square of the radius, the distant (lowest priority queues) will
</I>&gt;&gt;&gt;<i> have the
</I>&gt;&gt;&gt;<i> &gt; most objects while the highest priority queues will have a small number
</I>&gt;&gt;&gt;<i> of
</I>&gt;&gt;&gt;<i> &gt; objects. Inside each priority queue, we order the updates by the time
</I>&gt;&gt;&gt;<i> in
</I>&gt;&gt;&gt;<i> &gt; which they entered the queue. Then we pull a fixed number of updates
</I>&gt;&gt;&gt;<i> from
</I>&gt;&gt;&gt;<i> &gt; each priority queue each iteration. The result is that local updates
</I>&gt;&gt;&gt;<i> get a
</I>&gt;&gt;&gt;<i> &gt; high fraction of the outgoing bandwidth but distant updates are
</I>&gt;&gt;&gt;<i> guaranteed
</I>&gt;&gt;&gt;<i> &gt; to get at least &quot;some&quot; of the bandwidth. No starvation. The current
</I>&gt;&gt;&gt;<i> &gt; prioritization algorithm we implemented is a modification of the &quot;best
</I>&gt;&gt;&gt;<i> &gt; avatar responsiveness&quot; and &quot;front back&quot; in that we use root prim
</I>&gt;&gt;&gt;<i> location
</I>&gt;&gt;&gt;<i> &gt; for child prims and the priority of updates &quot;in back&quot; of the avatar is
</I>&gt;&gt;&gt;<i> lower
</I>&gt;&gt;&gt;<i> &gt; than updates &quot;in front&quot;. Our experiments show that the fair queuing
</I>&gt;&gt;&gt;<i> does
</I>&gt;&gt;&gt;<i> &gt; drain the update queue AND continues to provide a disproportionately
</I>&gt;&gt;&gt;<i> high
</I>&gt;&gt;&gt;<i> &gt; percentage of the bw to &quot;close&quot; updates.
</I>&gt;&gt;&gt;<i> &gt; One other note on this... we should be able to improve the performance
</I>&gt;&gt;&gt;<i> of
</I>&gt;&gt;&gt;<i> &gt; reprioritization with this approach. If we know the distance an avatar
</I>&gt;&gt;&gt;<i> has
</I>&gt;&gt;&gt;<i> &gt; moved, we only have to reprioritize objects that might have changed
</I>&gt;&gt;&gt;<i> priority
</I>&gt;&gt;&gt;<i> &gt; queues. Haven't implemented this yet but have some ideas for how to do
</I>&gt;&gt;&gt;<i> it.
</I>&gt;&gt;&gt;<i> &gt; * The resend queue is evil. When an update packet is sent (they are
</I>&gt;&gt;&gt;<i> marked
</I>&gt;&gt;&gt;<i> &gt; reliable) it is moved to a queue to await acknowledgement. If no
</I>&gt;&gt;&gt;<i> &gt; acknowledgement is received (in time), the packet is retransmitted and
</I>&gt;&gt;&gt;<i> the
</I>&gt;&gt;&gt;<i> &gt; wait time is doubled and so on... What that means is that a resend
</I>&gt;&gt;&gt;<i> packets
</I>&gt;&gt;&gt;<i> &gt; in a scene that is rapidly changing will often contain updates that are
</I>&gt;&gt;&gt;<i> &gt; outdated. That is, when we resend the packet, we are just resending old
</I>&gt;&gt;&gt;<i> data
</I>&gt;&gt;&gt;<i> &gt; (and if you're having a lot of resends that means you already have a
</I>&gt;&gt;&gt;<i> bad
</I>&gt;&gt;&gt;<i> &gt; connection &amp; now you're filling it up with useless data).
</I>&gt;&gt;&gt;<i> &gt; Fix: this isn't implemented yet (help would be appreciated)... we think
</I>&gt;&gt;&gt;<i> that
</I>&gt;&gt;&gt;<i> &gt; instead of saving packets for resend... a better solution would be to
</I>&gt;&gt;&gt;<i> keep
</I>&gt;&gt;&gt;<i> &gt; the entity updates that went into the packet. if we don't receive an
</I>&gt;&gt;&gt;<i> ack in
</I>&gt;&gt;&gt;<i> &gt; time, then put the entity updates back into the entity update queue
</I>&gt;&gt;&gt;<i> (with
</I>&gt;&gt;&gt;<i> &gt; entry time from their original enqueuing). That would ensure that we
</I>&gt;&gt;&gt;<i> send an
</I>&gt;&gt;&gt;<i> &gt; update for the object &amp; that the data sent is the most recent.
</I>&gt;&gt;&gt;<i> &gt; * One final note... per client bandwidth throttles seem to work very
</I>&gt;&gt;&gt;<i> well.
</I>&gt;&gt;&gt;<i> &gt; however, our experiments with per-simulator throttles was not positive.
</I>&gt;&gt;&gt;<i> it
</I>&gt;&gt;&gt;<i> &gt; appeared that a small number of clients was consuming all of the bw
</I>&gt;&gt;&gt;<i> &gt; available to the simulator and the rest were starved. Haven't looked
</I>&gt;&gt;&gt;<i> into
</I>&gt;&gt;&gt;<i> &gt; this any more.
</I>&gt;&gt;&gt;<i> &gt;
</I>&gt;&gt;&gt;<i> &gt; So...
</I>&gt;&gt;&gt;<i> &gt; Feedback appreciated... there is some logging code (disabled) in the
</I>&gt;&gt;&gt;<i> branch;
</I>&gt;&gt;&gt;<i> &gt; real data would be great. And help testing. there are a number of
</I>&gt;&gt;&gt;<i> &gt; attachment, deletes and so on that i'm not sure work correctly.
</I>&gt;&gt;&gt;<i> &gt; --mic
</I>&gt;&gt;&gt;<i> &gt;
</I>&gt;&gt;&gt;<i> &gt;
</I>&gt;&gt;&gt;<i> &gt;
</I>&gt;&gt;&gt;<i> &gt;
</I>&gt;&gt;&gt;<i> &gt;
</I>&gt;&gt;&gt;<i> &gt; _______________________________________________
</I>&gt;&gt;&gt;<i> &gt; Opensim-dev mailing list
</I>&gt;&gt;&gt;<i> &gt; <A HREF="https://lists.berlios.de/mailman/listinfo/opensim-dev">Opensim-dev at lists.berlios.de</A>
</I>&gt;&gt;&gt;<i> &gt; <A HREF="https://lists.berlios.de/mailman/listinfo/opensim-dev">https://lists.berlios.de/mailman/listinfo/opensim-dev</A>
</I>&gt;&gt;&gt;<i> &gt;
</I>&gt;&gt;&gt;<i> &gt;
</I>&gt;&gt;&gt;<i> _______________________________________________
</I>&gt;&gt;&gt;<i> Opensim-dev mailing list
</I>&gt;&gt;&gt;<i> <A HREF="https://lists.berlios.de/mailman/listinfo/opensim-dev">Opensim-dev at lists.berlios.de</A>
</I>&gt;&gt;&gt;<i> <A HREF="https://lists.berlios.de/mailman/listinfo/opensim-dev">https://lists.berlios.de/mailman/listinfo/opensim-dev</A>
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> _______________________________________________
</I>&gt;&gt;<i> Opensim-dev mailing list
</I>&gt;&gt;<i> <A HREF="https://lists.berlios.de/mailman/listinfo/opensim-dev">Opensim-dev at lists.berlios.de</A>
</I>&gt;&gt;<i> <A HREF="https://lists.berlios.de/mailman/listinfo/opensim-dev">https://lists.berlios.de/mailman/listinfo/opensim-dev</A>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;<i>
</I>&gt;<i> _______________________________________________
</I>&gt;<i> Opensim-dev mailing list
</I>&gt;<i> <A HREF="https://lists.berlios.de/mailman/listinfo/opensim-dev">Opensim-dev at lists.berlios.de</A>
</I>&gt;<i> <A HREF="https://lists.berlios.de/mailman/listinfo/opensim-dev">https://lists.berlios.de/mailman/listinfo/opensim-dev</A>
</I>&gt;<i>
</I>&gt;<i>
</I>-------------- next part --------------
An HTML attachment was scrubbed...
URL: &lt;<A HREF="https://lists.berlios.de/pipermail/opensim-dev/attachments/20110328/b524d3ed/attachment.html">https://lists.berlios.de/pipermail/opensim-dev/attachments/20110328/b524d3ed/attachment.html</A>&gt;
</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="010077.html">[Opensim-dev] networking issues
</A></li>
	<LI>Next message: <A HREF="010066.html">[Opensim-dev] networking issues
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#10079">[ date ]</a>
              <a href="thread.html#10079">[ thread ]</a>
              <a href="subject.html#10079">[ subject ]</a>
              <a href="author.html#10079">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/opensim-dev">More information about the Opensim-dev
mailing list</a><br>
</body></html>
